Purpose
-------
This file gives an AI coding agent the minimal, actionable knowledge to be productive in AUFalkon.

Quick run & CI commands
- Run all missions and generate headless reports: `python src/run_all_missions_ci.py --missions_glob "missions/**/mission*.json" --out_root ci_runs`
- Run a single mission headless: `python src/mission_runner.py <mission.json> --ticks 200 --logs_dir runner_logs`
- Validate a mission JSON: `python src/mission_validator.py missions/...? --capacity 2`

Core architecture (big picture)
- Scheduling core: `DeadlineScheduler` (see [src/scheduler_deadline.py](src/scheduler_deadline.py#L1)). This is the runtime engine that performs per-tick assignment, battery drain/recharge, rotation, and hard gap enforcement.
- Runner/CI: `src/mission_runner.py` and `src/run_all_missions_ci.py` drive simulations headlessly and produce logs used by `src/report_builder.py` (see [src/mission_runner.py](src/mission_runner.py#L1) and [src/run_all_missions_ci.py](src/run_all_missions_ci.py#L1)).
- Validation: `src/mission_validator.py` normalizes mission JSON semantics (notably the required_active_per_domain and mandatory `rest` domain).

Important project-specific conventions & semantics
- Missions are JSON files under `missions/` with keys: `units`, `domains` (must include `rest`), `tick_ms`, `constraints.max_gap_ms`, optional `required_active_per_domain`, `domain_pools`, `universal_roles`, and `domain_weights`.
- `rest` domain is reporting-only (not scheduled) and affects recharge weighting. See `mission_validator` and scheduler rest handling: [src/mission_validator.py](src/mission_validator.py#L1), [src/scheduler_deadline.py](src/scheduler_deadline.py#L1).
- Battery semantics: battery is a percent; when it reaches 0 the unit is permanently dead (never recharges). Domain weights scale drain and rest recharge: see `domain_weights` in `DeadlineScheduler`.
- Distinctness policy: scheduler prefers distinct devices for roles when possible; multi-role assignments happen only when distinctness cannot be met. This logic and events (e.g., `distinctness_wake`, `unmet_requirements`) live in `DeadlineScheduler`.
- Gap/mode semantics: `max_gap_ms` → `max_gap_ticks` and `strict_mission_failure` controls whether the scheduler raises on prolonged unmet requirements.

Logging & outputs (where to look)
- Per-run logs directory (`--logs_dir`): contains `timeline.csv`, `battery_samples.csv`, `assignment_samples.csv`, `events.csv`, and `summary.json` (written on scheduler close). See [src/scheduler_deadline.py](src/scheduler_deadline.py#L1).
- CI run metadata: `run_meta.json` written by `src/run_all_missions_ci.py` into each run folder.
- Final reports: generated by `src/report_builder.py` into each run subdirectory; `run_all_missions_ci.py` also creates `ci_runs/index.html` linking reports.

Common development tasks & examples
- To reproduce a failing CI mission locally: run the `mission_runner` with the mission file and inspect `events.csv` / `timeline.csv` / `summary.json`.
- To change scheduling knobs (e.g., rotation, capacity, thresholds) — prefer modifying arguments passed into `DeadlineScheduler` at creation sites (see `mission_runner.py` for typical parameterization).

Where to look when changing behavior
- Assignment, rotation, distinctness, battery updates, and mission-failure logic: [src/scheduler_deadline.py](src/scheduler_deadline.py#L1).
- Mission normalization and pre-checks: [src/mission_validator.py](src/mission_validator.py#L1).
- CI orchestration and report generation: [src/run_all_missions_ci.py](src/run_all_missions_ci.py#L1) and [src/report_builder.py](src/report_builder.py#L1).

Style & small rules for agents
- Keep changes minimally scoped and preserve existing public function signatures where possible.
- When adding runtime knobs, expose them in `mission_runner`/`run_all_missions_ci.py` first so CI can exercise new settings.
- Tests: there is no formal test harness in the repo—use the headless runner + mission JSONs as integration checks.

If anything here is unclear or you want more detail (examples of mission JSON keys or common failure modes), tell me which section to expand.

Example mission JSONs
---------------------
1) Minimal scalar requirements (applies same count to all non-rest domains):

```json
{
	"units": ["u01","u02","u03"],
	"domains": ["radar","comm","rest"],
	"tick_ms": 1.0,
	"constraints": { "max_gap_ms": 500 },
	"required_active_per_domain": 1
}
```

2) Per-domain requirements with pools and weights:

```json
{
	"units": ["u01","u02","u03","u04"],
	"domains": ["radar","comm","rest"],
	"tick_ms": 5.0,
	"mission_window_ms": 60000,
	"constraints": { "max_gap_ms": 2000 },
	"required_active_per_domain": { "radar": 1, "comm": 1, "rest": 0 },
	"domain_pools": { "radar": ["u01","u02"], "spares": ["u03","u04"] },
	"domain_weights": { "radar": 1.25, "comm": 0.9, "rest": 1.0 }
}
```

3) Example failure_injections (unit_crash permanent and temporary):

```json
{
	"scenario": "gap_failure",
	"failure_injections": [
		{"type": "unit_crash", "unit": "u02", "at_ms": 1000, "duration_ms": null /* permanent */}
	]
}
```

Troubleshooting & common fixes
------------------------------
- Missing `rest` domain: `mission.domains` must include `rest` (case-insensitive). Validator (`src/mission_validator.py`) rejects missions without it.
- CI can't find audit script: ensure `src/mission_injection_audit.py` exists and is tracked. In CI, add a debug step: `ls -la src hooks missions` to confirm files are present.
- Workflow not running: GitHub Actions only runs files under `.github/workflows/`. Move/rename any workflow to `.github/workflows/gui_sim.yml`.
- `mission_runner` raises on gap exceed: scheduler enforces `strict_mission_failure` (default True). To debug, run locally with a smaller tick cap or set `strict_mission_failure=False` when constructing `DeadlineScheduler` in tests.
- Interpreting logs:
	- `timeline.csv`: rows when assignments change (per-domain active devices).
	- `battery_samples.csv`: per-unit battery traces and states (`active`, `rest`, `down`, `dead`). Look for `battery_dead` events in `events.csv`.
	- `events.csv`: emits `unmet_requirements`, `mission_failure`, `low_battery_active`, `distinctness_wake`, etc.
	- `summary.json`: run-level metrics (distinct_ok_pct, multi_role_pct, battery_dead_units).
- Report generation: if `src/report_builder.py` fails, inspect the run folder contents (`ci_runs/<mission>/`) for the CSVs and `run_meta.json`; `report_builder.py` expects these inputs.
- Global CI budget: `run_all_missions_ci.py` derives per-mission ticks from `mission_window_ms / tick_ms` when present; otherwise use `--default_ticks`. Ensure mission JSONs set `mission_window_ms` where intended.

Next steps
----------
- If you want, I can:
	- (A) Expand the mission JSON examples with more scenarios (jitter, rotation, recharge_priority).
	- (B) Add a short CI workflow file at `.github/workflows/gui_sim.yml` wiring the validator, audit, run-all, and artifact uploads (fits within a 59-minute budget).

